{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a996b79-a31a-4f91-a546-a8fde487e8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.46.0-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.1 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/44.1 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/44.1 kB 330.3 kB/s eta 0:00:01\n",
      "     ----------------------------------- -- 41.0/44.1 kB 393.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 44.1/44.1 kB 271.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\limay\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.4)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.5.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\limay\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\limay\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\limay\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\limay\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\limay\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\limay\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\limay\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\limay\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\limay\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\limay\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\limay\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\limay\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\limay\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\limay\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\limay\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\limay\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.20.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\limay\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\limay\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\limay\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\limay\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\limay\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\limay\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\limay\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.6.2)\n",
      "Using cached sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "Using cached huggingface_hub-0.26.1-py3-none-any.whl (447 kB)\n",
      "Using cached torch-2.5.0-cp312-cp312-win_amd64.whl (203.1 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Downloading transformers-4.46.0-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.0 MB 1.3 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.2/10.0 MB 3.5 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.3/10.0 MB 12.1 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.9/10.0 MB 13.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.2/10.0 MB 15.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.5/10.0 MB 17.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.2/10.0 MB 18.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.5/10.0 MB 18.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.3/10.0 MB 18.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.4/10.0 MB 19.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.5/10.0 MB 19.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 19.4 MB/s eta 0:00:00\n",
      "Using cached safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Using cached tokenizers-0.20.1-cp312-none-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: sympy, safetensors, torch, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "Successfully installed huggingface-hub-0.26.1 safetensors-0.4.5 sentence-transformers-3.2.1 sympy-1.13.1 tokenizers-0.20.1 torch-2.5.0 transformers-4.46.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1d06487-9ede-4d68-87c1-74ccaff3ae3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limay\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful!\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np \n",
    "\n",
    "#Simple test to ensure everything works\n",
    "print(\"✓ Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6afd7d1-bc6d-48a9-969f-e21f8bf294d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 10 notes\n"
     ]
    }
   ],
   "source": [
    "#Creating our notes dataset\n",
    "# Initialize our collection of notes (Random notes)\n",
    "notes = [\n",
    "    \"Python lists are mutable sequences used to store collections of items. They can contain mixed types and are defined using square brackets.\",\n",
    "    \"Lists in Python can be modified after creation. Common operations include append(), extend(), and insert().\",\n",
    "    \"Data structures are fundamental building blocks in programming. They help organize and store data efficiently.\",\n",
    "    \"Arrays in NumPy provide efficient storage and operations for numerical data. They are widely used in scientific computing.\",\n",
    "    \"Object-oriented programming in Python uses classes and objects. Classes define the structure and behavior of objects.\",\n",
    "    \"The pandas library is built on top of NumPy and provides powerful data manipulation tools through DataFrames.\",\n",
    "    \"Version control with Git helps track changes in code. Common commands include commit, push, and pull.\",\n",
    "    \"Python functions are defined using the def keyword. They can accept parameters and return values.\",\n",
    "    \"Montreal is a multi-cultural city\",\n",
    "    \"Peru offers an extreme variety of ethnicities with amazing culture and cultural heritage\"\n",
    "]\n",
    "\n",
    "# Print the number of notes we have\n",
    "print(f\"Created {len(notes)} notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd2bbfb-a749-49f7-b275-88bc61b461a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing our dataset\n",
    "# Print each note with its length\n",
    "for i, note in enumerate(notes): #if we dont write enumerate we dont have index (i)\n",
    "    print(f\"\\nNote {i+1} (Length: {len(note)} characters):\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a86412b4-9333-4697-9a5a-7011eff25062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average words per note: 15.4\n",
      "Shortest note: 5 words\n",
      "Longest note: 22 words\n"
     ]
    }
   ],
   "source": [
    "#Comprehension of list of loops in just one line without appending every single time\n",
    "# Quick analysis of our notes\n",
    "note_lengths = [len(note.split()) for note in notes]\n",
    "# note.split() breaks each note (string) into a list of words by splitting it at spaces.\n",
    "# len(note.split()) calculates the number of words in each note.\n",
    "# The result is a list, note_lengths, where each entry corresponds to the word count of a note.\n",
    "\n",
    "print(f\"Average words per note: {sum(note_lengths)/len(note_lengths):.1f}\")\n",
    "print(f\"Shortest note: {min(note_lengths)} words\")\n",
    "print(f\"Longest note: {max(note_lengths)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1257c2e4-33c6-4103-9cd1-488d0a0f2e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model - this may take a few seconds\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bafb5bb1-8616-4255-afd3-e6aa872f0caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (384,)\n",
      "First 5 values: [ 0.03313288 -0.0281372   0.10922699  0.02421217  0.04646194]\n"
     ]
    }
   ],
   "source": [
    "# UNDERSTANDING EMBEDDINGS\n",
    "# Create an embedding for a simple sentence\n",
    "test_sentence = \"This is a test sentence to understand embeddings.\"\n",
    "embedding = model.encode(test_sentence)\n",
    "\n",
    "# Look at the embedding's properties\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"First 5 values: {embedding[:5]}\")\n",
    "\n",
    "#Each embedding is a vector of 384 numbers that represents the semantic meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4f4528f-916c-40ab-9c00-6ce033992470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings: 3\n",
      "Shape of each embedding: (384,)\n"
     ]
    }
   ],
   "source": [
    "#TESTING BATCH PROCESSING\n",
    "test_sentences = [\n",
    "    \"Python is a programming language\",\n",
    "    \"Programming languages are used to write software\",\n",
    "    \"Pythons are large snakes\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for all sentences at once\n",
    "embeddings = model.encode(test_sentences)\n",
    "\n",
    "print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "print(f\"Shape of each embedding: {embeddings[0].shape}\")\n",
    "#The output without the comma would be an int. Only () would still being tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14ef841d-eaea-4a6d-9d6e-e87f794ba411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity between:\n",
      "'Python is a programming language' and\n",
      "'Programming languages are used to write software':\n",
      "0.497\n",
      "\n",
      "Similarity between:\n",
      "'Python is a programming language' and\n",
      "'Pythons are large snakes':\n",
      "0.497\n",
      "\n",
      "Similarity between:\n",
      "'Programming languages are used to write software' and\n",
      "'Pythons are large snakes':\n",
      "0.497\n"
     ]
    }
   ],
   "source": [
    "#TESTING SIMILARITY\n",
    "import numpy as np\n",
    "\n",
    "# Calculate similarities between sentences\n",
    "def calculate_similarity(emb1, emb2):\n",
    "    return np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "\n",
    "# Get similarities between all pairs\n",
    "for i in range(len(test_sentences)): #To loop through each item of \"test_sentences\"\n",
    "    for j in range(i + 1, len(test_sentences)):\n",
    "        similarity = calculate_similarity(embeddings[2], embeddings[0]) #Compare between each sentence\n",
    "        print(f\"\\nSimilarity between:\\n'{test_sentences[i]}' and\\n'{test_sentences[j]}':\\n{similarity:.3f}\")\n",
    "# print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5705978-2bfb-49e3-9f5a-0c6763dee69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings for 10 notes\n",
      "Each embedding has shape: (384,)\n"
     ]
    }
   ],
   "source": [
    "# Convert all notes to embeddings\n",
    "note_embeddings = model.encode(notes)\n",
    "\n",
    "print(f\"Created embeddings for {len(notes)} notes\")\n",
    "print(f\"Each embedding has shape: {note_embeddings[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d2759c3-bac5-4c60-9f82-a12f3dd9ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_notes(query, top_k=3):\n",
    "    # Convert search query to embedding\n",
    "    query_embedding = model.encode(query)\n",
    "    \n",
    "    # Calculate similarities with all notes\n",
    "    similarities = np.dot(note_embeddings, query_embedding) / (\n",
    "        np.linalg.norm(note_embeddings, axis=1) * np.linalg.norm(query_embedding)\n",
    "    )\n",
    "    \n",
    "    # Get top k matches\n",
    "    top_idx = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    # A list of dictionaries\n",
    "    results = []\n",
    "    for idx in top_idx:\n",
    "        results.append({\n",
    "            'note': notes[idx],\n",
    "            'similarity': similarities[idx]\n",
    "        })\n",
    "        \n",
    "    # Return matching notes with thier similarity scores\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1eb114cf-e3d3-4e24-ab96-2c85a0a5db02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Query: 'Places that have extreme culture'\n",
      "==================================================\n",
      "\n",
      "1. Match (48.76% similar):\n",
      "Peru offers an extreme variety of ethnicities with amazing culture and cultural heritage\n",
      "\n",
      "2. Match (41.51% similar):\n",
      "Montreal is a multi-cultural city\n",
      "\n",
      "3. Match (2.54% similar):\n",
      "The pandas library is built on top of NumPy and provides powerful data manipulation tools through DataFrames.\n"
     ]
    }
   ],
   "source": [
    "#Trying some searches\n",
    "# Function to display search results nicely\n",
    "def display_results(query, results):\n",
    "    print(f\"\\nSearch Query: '{query}'\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Match ({r['similarity']:.2%} similar):\")\n",
    "        print(r['note'])\n",
    "\n",
    "# Try some example searches\n",
    "queries = [\n",
    "    \"Places that have extreme culture\"\n",
    "    # \"How do Python lists work?\",\n",
    "    # \"Tell me about data structures\",\n",
    "    # \"What is object oriented programming?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    results = search_notes(query)\n",
    "    display_results(query, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a6585-b8f3-4f83-a4fd-479b1b9e2c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
